# Visualising Pianists' Touch (VPT): Transcribing Expressive Piano Performance from Audio to Piano Key Motion
[![Paper](https://img.shields.io/badge/Paper-CHI%202026-green)](https://pseudo.link/paper)
[![Pre-trained Models](https://img.shields.io/badge/Models-Zenodo-9cf?logo=zenodo)](https://pseudo.link/models)
[![Code](https://img.shields.io/badge/Code-GitHub-black?logo=github)](https://pseudo.link/code)
[![Project Page](https://img.shields.io/badge/Demo-Project%20Page-blueviolet)](https://pseudo.link/project-page)
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey)](https://pseudo.link/license)

This repository contains the official implementation of our CHI 2026 paper:

**"Visualising Pianists’ Touch: Transcribing Expressive Piano Performance from Audio to Piano Key Motion"**

by Jingjing Tang, Shinichi Furuya, Hayato Nishioka, Momoko Shioki, Geraint Wiggins, György Fazekas, and Vincent K.M. Cheung.

## Overview
Understanding expressive piano performance requires not only *what* notes are played, but also *how* they are physically produced. MIDI provides a discrete event-based representation (pitch, onset, duration, velocity), but it cannot capture continuous touch gestures such as fine-grained key press depth trajectories.

VPT introduces an audio-to-key-motion transcription technique that predicts **continuous piano key motion trajectories directly from performance audio**, making motor-grounded performance information accessible without requiring specialised sensors.

## Details
To be released soon.

## Contact
Jingjing Tang: `jingjing.tang@qmul.ac.uk`

## License
This project is released under the Creative Commons Attribution 4.0 International License (CC BY 4.0).
